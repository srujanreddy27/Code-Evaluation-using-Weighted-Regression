{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fbg4zIur1Wjw",
    "outputId": "31d31cfe-ec9e-491a-b4b9-007f032c41df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from itertools import product\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    keywords = df[[f'Keywords_Vector_{i}' for i in range(20)]]\n",
    "    custom_data_structures = df[[f'Custom_Data_Structures_Vector_{i}' for i in range(20)]]\n",
    "    final_marks = df[\"Final_Marks\"]\n",
    "    return keywords, custom_data_structures, final_marks\n",
    "\n",
    "# Function to pad vectors with zeros to match length\n",
    "def pad_with_zeros(vector, max_length):\n",
    "    vector = np.atleast_2d(vector)\n",
    "    if vector.shape[1] < max_length:\n",
    "        padding = np.zeros((vector.shape[0], max_length - vector.shape[1]))\n",
    "        vector = np.hstack((vector, padding))\n",
    "    return vector\n",
    "\n",
    "# Reduction and combination strategies\n",
    "def combine_vectors(keywords, custom_data_structures, weights, fusion):\n",
    "    weighted_keywords = weights[1] * keywords\n",
    "    weighted_custom_data_structures = weights[0] * custom_data_structures\n",
    "\n",
    "    if fusion == \"add\":\n",
    "        max_length = max(weighted_keywords.shape[1], weighted_custom_data_structures.shape[1])\n",
    "        weighted_keywords = pad_with_zeros(weighted_keywords, max_length)\n",
    "        weighted_custom_data_structures = pad_with_zeros(weighted_custom_data_structures, max_length)\n",
    "        combined_vectors = weighted_keywords + weighted_custom_data_structures\n",
    "    elif fusion == \"concatenate\":\n",
    "        combined_vectors = np.hstack((weighted_keywords, weighted_custom_data_structures))\n",
    "    return combined_vectors\n",
    "\n",
    "# CatBoost parameter options\n",
    "iterations_list = [1500, 1000, 2000]\n",
    "learning_rate_list = [0.01, 0.05]\n",
    "l2_leaf_reg_list = [5, 7]\n",
    "early_stopping_rounds_list = [15, 30]\n",
    "\n",
    "# Generate all combinations of CatBoost parameters\n",
    "param_combinations = list(product(iterations_list, learning_rate_list, l2_leaf_reg_list, early_stopping_rounds_list))\n",
    "\n",
    "# Load data\n",
    "file_path = \"pca.xlsx\"\n",
    "keywords, custom_data_structures, final_marks = load_data(file_path)\n",
    "\n",
    "# Weights and fusion types\n",
    "weights_options = [(1, 1), (1,9),(9,1),(2,8),(8,2),(3,7),(7,3),(4,6),(6,4),(0.1,0.9),(0.9,0.1),(0.2,0.8),(0.8,0.2),(0.3,0.7),(0.7,0.3),(0.4,0.6),(0.6,0.4)]\n",
    "fusions = [\"add\", \"concatenate\"]\n",
    "\n",
    "# DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate models with different combinations\n",
    "for weights in weights_options:\n",
    "    for fusion in fusions:\n",
    "        combined_vectors = combine_vectors(keywords, custom_data_structures, weights, fusion)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(combined_vectors, final_marks, test_size=0.2, random_state=42, stratify=final_marks)\n",
    "\n",
    "        for params in param_combinations:\n",
    "            iterations, learning_rate, l2_leaf_reg, early_stopping_rounds = params\n",
    "\n",
    "            # Print parameters being tested\n",
    "            print(f\"Testing CatBoost with iterations={iterations}, learning_rate={learning_rate}, l2_leaf_reg={l2_leaf_reg}, early_stopping_rounds={early_stopping_rounds}\")\n",
    "\n",
    "            model = CatBoostRegressor(\n",
    "                iterations=iterations,\n",
    "                learning_rate=learning_rate,\n",
    "                l2_leaf_reg=l2_leaf_reg,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                random_state=43,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Train metrics\n",
    "            y_train_pred = np.round(model.predict(X_train))\n",
    "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "            train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "            train_rmse = np.sqrt(train_mse)\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "\n",
    "            # Test metrics\n",
    "            y_test_pred = np.round(model.predict(X_test))\n",
    "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "            test_rmse = np.sqrt(test_mse)\n",
    "            test_r2 = r2_score(y_test, y_test_pred)\n",
    "            test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"Model\": \"CatBoost Regressor\",\n",
    "                \"Weights\": weights,\n",
    "                \"Fusion\": fusion,\n",
    "                \"Iterations\": iterations,\n",
    "                \"Learning Rate\": learning_rate,\n",
    "                \"L2 Leaf Reg\": l2_leaf_reg,\n",
    "                \"Early Stopping Rounds\": early_stopping_rounds,\n",
    "                \"Train MAE\": train_mae,\n",
    "                \"Train MSE\": train_mse,\n",
    "                \"Train RMSE\": train_rmse,\n",
    "                \"Train R2\": train_r2,\n",
    "                \"Train MAPE\": train_mape,\n",
    "                \"Test MAE\": test_mae,\n",
    "                \"Test MSE\": test_mse,\n",
    "                \"Test RMSE\": test_rmse,\n",
    "                \"Test R2\": test_r2,\n",
    "                \"Test MAPE\": test_mape\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort results by Fusion and Test R2 in descending order\n",
    "results_df.sort_values(by=[\"Fusion\", \"Test R2\"], ascending=[True, False], inplace=True)\n",
    "# Save results to Excel\n",
    "results_df.to_excel(\"catboostpca2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=3, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=3, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=3, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=3, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=3, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=3, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.01, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=3, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=3, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=3, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=5, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=2000, learning_rate=0.05, l2_leaf_reg=7, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=20\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=3, early_stopping_rounds=30\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=15\n",
      "Testing CatBoost with iterations=1500, learning_rate=0.01, l2_leaf_reg=5, early_stopping_rounds=20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from itertools import product\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    keywords = df[[f'Keywords_Vector_{i}' for i in range(20)]]\n",
    "    custom_data_structures = df[[f'Custom_Data_Structures_Vector_{i}' for i in range(20)]]\n",
    "    final_marks = df[\"Final_Marks\"]\n",
    "    return keywords, custom_data_structures, final_marks\n",
    "\n",
    "# Function to pad vectors with zeros to match length\n",
    "def pad_with_zeros(vector, max_length):\n",
    "    vector = np.atleast_2d(vector)\n",
    "    if vector.shape[1] < max_length:\n",
    "        padding = np.zeros((vector.shape[0], max_length - vector.shape[1]))\n",
    "        vector = np.hstack((vector, padding))\n",
    "    return vector\n",
    "\n",
    "# Reduction and combination strategies\n",
    "def combine_vectors(keywords, custom_data_structures, weights, fusion):\n",
    "    weighted_keywords = weights[1] * keywords\n",
    "    weighted_custom_data_structures = weights[0] * custom_data_structures\n",
    "\n",
    "    if fusion == \"add\":\n",
    "        max_length = max(weighted_keywords.shape[1], weighted_custom_data_structures.shape[1])\n",
    "        weighted_keywords = pad_with_zeros(weighted_keywords, max_length)\n",
    "        weighted_custom_data_structures = pad_with_zeros(weighted_custom_data_structures, max_length)\n",
    "        combined_vectors = weighted_keywords + weighted_custom_data_structures\n",
    "    elif fusion == \"concatenate\":\n",
    "        combined_vectors = np.hstack((weighted_keywords, weighted_custom_data_structures))\n",
    "    return combined_vectors\n",
    "\n",
    "# CatBoost parameter options\n",
    "iterations_list = [1500, 1000, 2000]\n",
    "learning_rate_list = [0.01, 0.05]\n",
    "l2_leaf_reg_list = [3, 5, 7]\n",
    "early_stopping_rounds_list = [15, 20, 30]\n",
    "\n",
    "# Generate all combinations of CatBoost parameters\n",
    "param_combinations = list(product(iterations_list, learning_rate_list, l2_leaf_reg_list, early_stopping_rounds_list))\n",
    "\n",
    "# Load data\n",
    "file_path = \"IG.xlsx\"\n",
    "keywords, custom_data_structures, final_marks = load_data(file_path)\n",
    "\n",
    "# Weights and fusion types\n",
    "weights_options = [(1, 1), (0.4, 0.6), (0.1, 0.9)]\n",
    "fusions = [\"add\", \"concatenate\"]\n",
    "\n",
    "# DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate models with different combinations\n",
    "for weights in weights_options:\n",
    "    for fusion in fusions:\n",
    "        combined_vectors = combine_vectors(keywords, custom_data_structures, weights, fusion)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(combined_vectors, final_marks, test_size=0.2, random_state=42, stratify=final_marks)\n",
    "\n",
    "        for params in param_combinations:\n",
    "            iterations, learning_rate, l2_leaf_reg, early_stopping_rounds = params\n",
    "\n",
    "            # Print parameters being tested\n",
    "            print(f\"Testing CatBoost with iterations={iterations}, learning_rate={learning_rate}, l2_leaf_reg={l2_leaf_reg}, early_stopping_rounds={early_stopping_rounds}\")\n",
    "\n",
    "            model = CatBoostRegressor(\n",
    "                iterations=iterations,\n",
    "                learning_rate=learning_rate,\n",
    "                l2_leaf_reg=l2_leaf_reg,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                random_state=43,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Train metrics\n",
    "            y_train_pred = np.round(model.predict(X_train))\n",
    "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "            train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "            train_rmse = np.sqrt(train_mse)\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "\n",
    "            # Test metrics\n",
    "            y_test_pred = np.round(model.predict(X_test))\n",
    "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "            test_rmse = np.sqrt(test_mse)\n",
    "            test_r2 = r2_score(y_test, y_test_pred)\n",
    "            test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"Model\": \"CatBoost Regressor\",\n",
    "                \"Weights\": weights,\n",
    "                \"Fusion\": fusion,\n",
    "                \"Iterations\": iterations,\n",
    "                \"Learning Rate\": learning_rate,\n",
    "                \"L2 Leaf Reg\": l2_leaf_reg,\n",
    "                \"Early Stopping Rounds\": early_stopping_rounds,\n",
    "                \"Train MAE\": train_mae,\n",
    "                \"Train MSE\": train_mse,\n",
    "                \"Train RMSE\": train_rmse,\n",
    "                \"Train R2\": train_r2,\n",
    "                \"Train MAPE\": train_mape,\n",
    "                \"Test MAE\": test_mae,\n",
    "                \"Test MSE\": test_mse,\n",
    "                \"Test RMSE\": test_rmse,\n",
    "                \"Test R2\": test_r2,\n",
    "                \"Test MAPE\": test_mape\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort results by Fusion and Test R2 in descending order\n",
    "results_df.sort_values(by=[\"Fusion\", \"Test R2\"], ascending=[True, False], inplace=True)\n",
    "# Save results to Excel\n",
    "results_df.to_excel(\"catboostactualIG.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuwkXGdwYOmO"
   },
   "source": [
    "For PCA reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ka8JzrmIwd9T",
    "outputId": "299c283b-dec9-411d-9cbe-88999e30e523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to pca_results.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, StackingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    keywords = df[[f'Keywords_Vector_{i}' for i in range(20)]]\n",
    "    custom_data_structures = df[[f'Custom_Data_Structures_Vector_{i}' for i in range(20)]]\n",
    "    final_marks = df[\"Final_Marks\"]\n",
    "    return keywords, custom_data_structures, final_marks\n",
    "\n",
    "# Function to pad vectors with zeros to match length\n",
    "def pad_with_zeros(vector, max_length):\n",
    "    vector = np.atleast_2d(vector)\n",
    "    if vector.shape[1] < max_length:\n",
    "        padding = np.zeros((vector.shape[0], max_length - vector.shape[1]))\n",
    "        vector = np.hstack((vector, padding))\n",
    "    return vector\n",
    "\n",
    "# Reduction and combination strategies\n",
    "def combine_vectors(keywords, custom_data_structures, weights, fusion):\n",
    "    weighted_keywords = weights[1] * keywords\n",
    "    weighted_custom_data_structures = weights[0] * custom_data_structures\n",
    "\n",
    "    if fusion == \"add\":\n",
    "        max_length = max(weighted_keywords.shape[1], weighted_custom_data_structures.shape[1])\n",
    "        weighted_keywords = pad_with_zeros(weighted_keywords, max_length)\n",
    "        weighted_custom_data_structures = pad_with_zeros(weighted_custom_data_structures, max_length)\n",
    "        combined_vectors = weighted_keywords + weighted_custom_data_structures\n",
    "    elif fusion == \"concatenate\":\n",
    "        combined_vectors = np.hstack((weighted_keywords, weighted_custom_data_structures))\n",
    "    return combined_vectors\n",
    "\n",
    "# Model definitions\n",
    "catboost_params = {\n",
    "    'verbose': 0,\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 5,\n",
    "    'early_stopping_rounds': 15, 'random_state': 43\n",
    "}\n",
    "\n",
    "xgboost_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 2,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1, 'random_state': 43\n",
    "}\n",
    "\n",
    "random_forest_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 3, 'random_state': 43\n",
    "}\n",
    "\n",
    "bagging_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_samples': 0.8,\n",
    "    'max_features': 1.0, 'random_state': 43\n",
    "}\n",
    "\n",
    "# Base models\n",
    "models = {\n",
    "    \"CatBoost Regressor\": CatBoostRegressor(**catboost_params),\n",
    "    \"XGBoost Regressor\": XGBRegressor(**xgboost_params),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(**random_forest_params),\n",
    "    \"Bagging Regressor\": BaggingRegressor(**bagging_params)\n",
    "}\n",
    "\n",
    "# Stacking regressor\n",
    "estimators = [(name, model) for name, model in models.items()]\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=CatBoostRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, verbose=0),\n",
    ")\n",
    "models[\"Stacking Regressor\"] = stacking_model\n",
    "\n",
    "# Load data\n",
    "file_path = \"/content/pca.xlsx\"\n",
    "keywords, custom_data_structures, final_marks = load_data(file_path)\n",
    "\n",
    "# Weights and fusion types\n",
    "weights_options = [(1, 1), (0.6, 0.4), (0.7, 0.3), (0.8, 0.2), (0.9, 0.1), (0.4, 0.6), (0.3, 0.7), (0.2, 0.8), (0.1, 0.9), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
    "fusions = [\"add\", \"concatenate\"]\n",
    "\n",
    "# DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate models\n",
    "for weights in weights_options:\n",
    "    for fusion in fusions:\n",
    "        combined_vectors = combine_vectors(keywords, custom_data_structures, weights, fusion)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(combined_vectors, final_marks, test_size=0.2, random_state=42, stratify=final_marks)\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Train metrics\n",
    "            y_train_pred = np.round(model.predict(X_train))\n",
    "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "            train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "            train_rmse = np.sqrt(train_mse)\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "\n",
    "            # Test metrics\n",
    "            y_test_pred = np.round(model.predict(X_test))\n",
    "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "            test_rmse = np.sqrt(test_mse)\n",
    "            test_r2 = r2_score(y_test, y_test_pred)\n",
    "            test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"Model\": name,\n",
    "                \"Weights\": weights,\n",
    "                \"Fusion\": fusion,\n",
    "                \"Train MAE\": train_mae,\n",
    "                \"Train MSE\": train_mse,\n",
    "                \"Train RMSE\": train_rmse,\n",
    "                \"Train R2\": train_r2,\n",
    "                \"Train MAPE\": train_mape,\n",
    "                \"Test MAE\": test_mae,\n",
    "                \"Test MSE\": test_mse,\n",
    "                \"Test RMSE\": test_rmse,\n",
    "                \"Test R2\": test_r2,\n",
    "                \"Test MAPE\": test_mape\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort results by fusion method and Test R2 in descending order\n",
    "results_df.sort_values(by=[\"Fusion\", \"Test R2\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "# Save results to Excel\n",
    "results_df.to_excel(\"pca_results.xlsx\", index=False)\n",
    "print(\"Results saved to pca_results.xlsx.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE5hkUHRO0sZ"
   },
   "source": [
    "For CHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0XqXSDvtxey",
    "outputId": "3cf27269-569a-4a75-db12-68fe061b8e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to chi_results.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, StackingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    keywords = df[[f'Keywords_Vector_{i}' for i in range(20)]]\n",
    "    custom_data_structures = df[[f'Custom_Data_Structures_Vector_{i}' for i in range(20)]]\n",
    "    final_marks = df[\"Final_Marks\"]\n",
    "    return keywords, custom_data_structures, final_marks\n",
    "\n",
    "# Function to pad vectors with zeros to match length\n",
    "def pad_with_zeros(vector, max_length):\n",
    "    vector = np.atleast_2d(vector)\n",
    "    if vector.shape[1] < max_length:\n",
    "        padding = np.zeros((vector.shape[0], max_length - vector.shape[1]))\n",
    "        vector = np.hstack((vector, padding))\n",
    "    return vector\n",
    "\n",
    "# Reduction and combination strategies\n",
    "def combine_vectors(keywords, custom_data_structures, weights, fusion):\n",
    "    weighted_keywords = weights[1] * keywords\n",
    "    weighted_custom_data_structures = weights[0] * custom_data_structures\n",
    "\n",
    "    if fusion == \"add\":\n",
    "        max_length = max(weighted_keywords.shape[1], weighted_custom_data_structures.shape[1])\n",
    "        weighted_keywords = pad_with_zeros(weighted_keywords, max_length)\n",
    "        weighted_custom_data_structures = pad_with_zeros(weighted_custom_data_structures, max_length)\n",
    "        combined_vectors = weighted_keywords + weighted_custom_data_structures\n",
    "    elif fusion == \"concatenate\":\n",
    "        combined_vectors = np.hstack((weighted_keywords, weighted_custom_data_structures))\n",
    "    return combined_vectors\n",
    "\n",
    "# Model definitions\n",
    "catboost_params = {\n",
    "    'verbose': 0,\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 5,\n",
    "    'early_stopping_rounds': 15, 'random_state': 43\n",
    "}\n",
    "\n",
    "xgboost_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 2,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1, 'random_state': 43\n",
    "}\n",
    "\n",
    "random_forest_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 3, 'random_state': 43\n",
    "}\n",
    "\n",
    "bagging_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_samples': 0.8,\n",
    "    'max_features': 1.0, 'random_state': 43\n",
    "}\n",
    "\n",
    "# Base models\n",
    "models = {\n",
    "    \"CatBoost Regressor\": CatBoostRegressor(**catboost_params),\n",
    "    \"XGBoost Regressor\": XGBRegressor(**xgboost_params),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(**random_forest_params),\n",
    "    \"Bagging Regressor\": BaggingRegressor(**bagging_params)\n",
    "}\n",
    "\n",
    "# Stacking regressor\n",
    "estimators = [(name, model) for name, model in models.items()]\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=CatBoostRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, verbose=0),\n",
    ")\n",
    "models[\"Stacking Regressor\"] = stacking_model\n",
    "\n",
    "# Load data\n",
    "file_path = \"/content/chi.xlsx\"\n",
    "keywords, custom_data_structures, final_marks = load_data(file_path)\n",
    "\n",
    "# Weights and fusion types\n",
    "weights_options = [(1, 1), (0.6, 0.4), (0.7, 0.3), (0.8, 0.2), (0.9, 0.1), (0.4, 0.6), (0.3, 0.7), (0.2, 0.8), (0.1, 0.9), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
    "fusions = [\"add\", \"concatenate\"]\n",
    "\n",
    "# DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate models\n",
    "for weights in weights_options:\n",
    "    for fusion in fusions:\n",
    "        combined_vectors = combine_vectors(keywords, custom_data_structures, weights, fusion)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(combined_vectors, final_marks, test_size=0.2, random_state=42, stratify=final_marks)\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Train metrics\n",
    "            y_train_pred = np.round(model.predict(X_train))\n",
    "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "            train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "            train_rmse = np.sqrt(train_mse)\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "\n",
    "            # Test metrics\n",
    "            y_test_pred = np.round(model.predict(X_test))\n",
    "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "            test_rmse = np.sqrt(test_mse)\n",
    "            test_r2 = r2_score(y_test, y_test_pred)\n",
    "            test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"Model\": name,\n",
    "                \"Weights\": weights,\n",
    "                \"Fusion\": fusion,\n",
    "                \"Train MAE\": train_mae,\n",
    "                \"Train MSE\": train_mse,\n",
    "                \"Train RMSE\": train_rmse,\n",
    "                \"Train R2\": train_r2,\n",
    "                \"Train MAPE\": train_mape,\n",
    "                \"Test MAE\": test_mae,\n",
    "                \"Test MSE\": test_mse,\n",
    "                \"Test RMSE\": test_rmse,\n",
    "                \"Test R2\": test_r2,\n",
    "                \"Test MAPE\": test_mape\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort results by fusion method and Test R2 in descending order\n",
    "results_df.sort_values(by=[\"Fusion\", \"Test R2\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "# Save results to Excel\n",
    "results_df.to_excel(\"chi_results.xlsx\", index=False)\n",
    "print(\"Results saved to chi_results.xlsx.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLB9zAV9YTmt"
   },
   "source": [
    "For Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqENy9u1YaKB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, StackingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    keywords = df[[f'Keywords_Vector_{i}' for i in range(20)]]\n",
    "    custom_data_structures = df[[f'Custom_Data_Structures_Vector_{i}' for i in range(20)]]\n",
    "    final_marks = df[\"Final_Marks\"]\n",
    "    return keywords, custom_data_structures, final_marks\n",
    "\n",
    "# Function to pad vectors with zeros to match length\n",
    "def pad_with_zeros(vector, max_length):\n",
    "    vector = np.atleast_2d(vector)\n",
    "    if vector.shape[1] < max_length:\n",
    "        padding = np.zeros((vector.shape[0], max_length - vector.shape[1]))\n",
    "        vector = np.hstack((vector, padding))\n",
    "    return vector\n",
    "\n",
    "# Reduction and combination strategies\n",
    "def combine_vectors(keywords, custom_data_structures, weights, fusion):\n",
    "    weighted_keywords = weights[1] * keywords\n",
    "    weighted_custom_data_structures = weights[0] * custom_data_structures\n",
    "\n",
    "    if fusion == \"add\":\n",
    "        max_length = max(weighted_keywords.shape[1], weighted_custom_data_structures.shape[1])\n",
    "        weighted_keywords = pad_with_zeros(weighted_keywords, max_length)\n",
    "        weighted_custom_data_structures = pad_with_zeros(weighted_custom_data_structures, max_length)\n",
    "        combined_vectors = weighted_keywords + weighted_custom_data_structures\n",
    "    elif fusion == \"concatenate\":\n",
    "        combined_vectors = np.hstack((weighted_keywords, weighted_custom_data_structures))\n",
    "    return combined_vectors\n",
    "\n",
    "# Model definitions\n",
    "catboost_params = {\n",
    "    'verbose': 0,\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 5,\n",
    "    'early_stopping_rounds': 15, 'random_state': 43\n",
    "}\n",
    "\n",
    "xgboost_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 2,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1, 'random_state': 43\n",
    "}\n",
    "\n",
    "random_forest_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 3, 'random_state': 43\n",
    "}\n",
    "\n",
    "bagging_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_samples': 0.8,\n",
    "    'max_features': 1.0, 'random_state': 43\n",
    "}\n",
    "\n",
    "# Base models\n",
    "models = {\n",
    "    \"CatBoost Regressor\": CatBoostRegressor(**catboost_params),\n",
    "    \"XGBoost Regressor\": XGBRegressor(**xgboost_params),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(**random_forest_params),\n",
    "    \"Bagging Regressor\": BaggingRegressor(**bagging_params)\n",
    "}\n",
    "\n",
    "# Stacking regressor\n",
    "estimators = [(name, model) for name, model in models.items()]\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=CatBoostRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, verbose=0),\n",
    ")\n",
    "models[\"Stacking Regressor\"] = stacking_model\n",
    "\n",
    "# Load data\n",
    "file_path = \"/content/IG.xlsx\"\n",
    "keywords, custom_data_structures, final_marks = load_data(file_path)\n",
    "\n",
    "# Weights and fusion types\n",
    "weights_options = [(1, 1), (0.6, 0.4), (0.7, 0.3), (0.8, 0.2), (0.9, 0.1), (0.4, 0.6), (0.3, 0.7), (0.2, 0.8), (0.1, 0.9), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
    "fusions = [\"add\", \"concatenate\"]\n",
    "\n",
    "# DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate models\n",
    "for weights in weights_options:\n",
    "    for fusion in fusions:\n",
    "        combined_vectors = combine_vectors(keywords, custom_data_structures, weights, fusion)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(combined_vectors, final_marks, test_size=0.2, random_state=42, stratify=final_marks)\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Train metrics\n",
    "            y_train_pred = np.round(model.predict(X_train))\n",
    "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "            train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "            train_rmse = np.sqrt(train_mse)\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "\n",
    "            # Test metrics\n",
    "            y_test_pred = np.round(model.predict(X_test))\n",
    "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "            test_rmse = np.sqrt(test_mse)\n",
    "            test_r2 = r2_score(y_test, y_test_pred)\n",
    "            test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"Model\": name,\n",
    "                \"Weights\": weights,\n",
    "                \"Fusion\": fusion,\n",
    "                \"Train MAE\": train_mae,\n",
    "                \"Train MSE\": train_mse,\n",
    "                \"Train RMSE\": train_rmse,\n",
    "                \"Train R2\": train_r2,\n",
    "                \"Train MAPE\": train_mape,\n",
    "                \"Test MAE\": test_mae,\n",
    "                \"Test MSE\": test_mse,\n",
    "                \"Test RMSE\": test_rmse,\n",
    "                \"Test R2\": test_r2,\n",
    "                \"Test MAPE\": test_mape\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort results by fusion method and Test R2 in descending order\n",
    "results_df.sort_values(by=[\"Fusion\", \"Test R2\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "# Save results to Excel\n",
    "results_df.to_excel(\"IG_results.xlsx\", index=False)\n",
    "print(\"Results saved to IG_results.xlsx.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yoqt3C62_5tN"
   },
   "source": [
    "For to 20 variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NziaBE_x_5Yi",
    "outputId": "9d0522ed-9cd1-4991-80ed-e9332b6990a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to var_results.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, StackingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    keywords = df[[f'Keywords_Vector_{i}' for i in range(20)]]\n",
    "    custom_data_structures = df[[f'Custom_Data_Structures_Vector_{i}' for i in range(20)]]\n",
    "    final_marks = df[\"Final_Marks\"]\n",
    "    return keywords, custom_data_structures, final_marks\n",
    "\n",
    "# Function to pad vectors with zeros to match length\n",
    "def pad_with_zeros(vector, max_length):\n",
    "    vector = np.atleast_2d(vector)\n",
    "    if vector.shape[1] < max_length:\n",
    "        padding = np.zeros((vector.shape[0], max_length - vector.shape[1]))\n",
    "        vector = np.hstack((vector, padding))\n",
    "    return vector\n",
    "\n",
    "# Reduction and combination strategies\n",
    "def combine_vectors(keywords, custom_data_structures, weights, fusion):\n",
    "    weighted_keywords = weights[1] * keywords\n",
    "    weighted_custom_data_structures = weights[0] * custom_data_structures\n",
    "\n",
    "    if fusion == \"add\":\n",
    "        max_length = max(weighted_keywords.shape[1], weighted_custom_data_structures.shape[1])\n",
    "        weighted_keywords = pad_with_zeros(weighted_keywords, max_length)\n",
    "        weighted_custom_data_structures = pad_with_zeros(weighted_custom_data_structures, max_length)\n",
    "        combined_vectors = weighted_keywords + weighted_custom_data_structures\n",
    "    elif fusion == \"concatenate\":\n",
    "        combined_vectors = np.hstack((weighted_keywords, weighted_custom_data_structures))\n",
    "    return combined_vectors\n",
    "\n",
    "# Model definitions\n",
    "catboost_params = {\n",
    "    'verbose': 0,\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 5,\n",
    "    'early_stopping_rounds': 15, 'random_state': 43\n",
    "}\n",
    "\n",
    "xgboost_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 2,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1, 'random_state': 43\n",
    "}\n",
    "\n",
    "random_forest_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 3, 'random_state': 43\n",
    "}\n",
    "\n",
    "bagging_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_samples': 0.8,\n",
    "    'max_features': 1.0, 'random_state': 43\n",
    "}\n",
    "\n",
    "# Base models\n",
    "models = {\n",
    "    \"CatBoost Regressor\": CatBoostRegressor(**catboost_params),\n",
    "    \"XGBoost Regressor\": XGBRegressor(**xgboost_params),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(**random_forest_params),\n",
    "    \"Bagging Regressor\": BaggingRegressor(**bagging_params)\n",
    "}\n",
    "\n",
    "# Stacking regressor\n",
    "estimators = [(name, model) for name, model in models.items()]\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=CatBoostRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, verbose=0),\n",
    ")\n",
    "models[\"Stacking Regressor\"] = stacking_model\n",
    "\n",
    "# Load data\n",
    "file_path = \"/content/var.xlsx\"\n",
    "keywords, custom_data_structures, final_marks = load_data(file_path)\n",
    "\n",
    "# Weights and fusion types\n",
    "weights_options = [(1, 1), (0.6, 0.4), (0.7, 0.3), (0.8, 0.2), (0.9, 0.1), (0.4, 0.6), (0.3, 0.7), (0.2, 0.8), (0.1, 0.9)]\n",
    "fusions = [\"add\", \"concatenate\"]\n",
    "\n",
    "# DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate models\n",
    "for weights in weights_options:\n",
    "    for fusion in fusions:\n",
    "        combined_vectors = combine_vectors(keywords, custom_data_structures, weights, fusion)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(combined_vectors, final_marks, test_size=0.2, random_state=42, stratify=final_marks)\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Train metrics\n",
    "            y_train_pred = np.round(model.predict(X_train))\n",
    "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "            train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "            train_rmse = np.sqrt(train_mse)\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "\n",
    "            # Test metrics\n",
    "            y_test_pred = np.round(model.predict(X_test))\n",
    "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "            test_rmse = np.sqrt(test_mse)\n",
    "            test_r2 = r2_score(y_test, y_test_pred)\n",
    "            test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"Model\": name,\n",
    "                \"Weights\": weights,\n",
    "                \"Fusion\": fusion,\n",
    "                \"Train MAE\": train_mae,\n",
    "                \"Train MSE\": train_mse,\n",
    "                \"Train RMSE\": train_rmse,\n",
    "                \"Train R2\": train_r2,\n",
    "                \"Train MAPE\": train_mape,\n",
    "                \"Test MAE\": test_mae,\n",
    "                \"Test MSE\": test_mse,\n",
    "                \"Test RMSE\": test_rmse,\n",
    "                \"Test R2\": test_r2,\n",
    "                \"Test MAPE\": test_mape\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort results by fusion method and Test R2 in descending order\n",
    "results_df.sort_values(by=[\"Fusion\", \"Test R2\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "# Save results to Excel\n",
    "results_df.to_excel(\"var_results.xlsx\", index=False)\n",
    "print(\"Results saved to var_results.xlsx.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
